DIRECTORY_PATH = "./data/"
INDEX_NAME = "test"

import io 
import json
import os
import zipfile
import streamlit as st
import feedparser
from langchain_core.documents import Document
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from google.oauth2 import service_account
from warnings import filterwarnings
from moviepy.editor import AudioFileClip
from concurrent.futures import ThreadPoolExecutor
from pydub import AudioSegment
import unicodedata
from openai import OpenAI
filterwarnings("ignore")
import time
import re
from dotenv import load_dotenv


SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
TEMP_DOWNLOAD_DIR = './temp_downloads'


class DocumentProcessor:
    def __init__(self, directory_path="./data/",
                index_name="test",
                drive_folder_id=None,
                rss_url = "https://feeds.simplecast.com/XFfCG1w8"):
        load_dotenv()
        self.rss_url = rss_url
        self.drive_folder_id = drive_folder_id
        self.dimensions = 1536
        self.directory = directory_path
        self.index_name = index_name
        # openai_api_key = os.getenv("OPENAI_API_KEY")
        openai_api_key = st.secrets['OPENAI_API_KEY']

        self.client = OpenAI(api_key=openai_api_key)
        self.embeddings = OpenAIEmbeddings(api_key=openai_api_key, model="text-embedding-3-small")
        self.vector_store = self.load_pinecone_vector_store()
        print("Document Processor initialized.")

    def load_pinecone_vector_store(self):
        # pinecone_api_key = os.getenv("PINECONE_API_KEY")
        pinecone_api_key = st.secrets['PINECONE_API_KEY']
        if not pinecone_api_key:
            raise ValueError("No Pinecone API key found in environment variables.")
        print("LOading {} index".format(self.index_name))
        pc = Pinecone(api_key=pinecone_api_key)

        if self.index_name not in pc.list_indexes().names():
            pc.create_index(
                name=self.index_name,
                dimension=1536,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
            while not pc.describe_index(self.index_name).status["ready"]:
                time.sleep(1)
                
        self.index = pc.Index(self.index_name)
        print(f"Pinecone vector store '{self.index_name}' loaded.")
        self.vector_store = PineconeVectorStore(index=self.index, embedding=self.embeddings)
        return self.vector_store
    

    def get_file_paths_from_directory_and_subdirectories(self):
        """
        Get all PDF and DOCX file paths from the directory,
        including files in nested directories.
        """
        file_paths = []
        for root, _, files in os.walk(self.directory):
            for filename in files:
                if filename.endswith((".pdf", ".docx")):
                    # Construct the full file path and add it to the list
                    file_paths.append(os.path.join(root, filename))
        return file_paths


    def check_existing_docs_by_id(self, doc_ids):
        """
        Check if the document IDs (filenames) exist in the Pinecone index.
        """
        # print(doc_ids)
        existing_ids = set()
        for doc_id in doc_ids:
            try:
                response = self.index.fetch(ids=[doc_id])
                # print(response)
                if response["vectors"]:
                    existing_ids.add(doc_id)
            except Exception as e:
                print(f"Error checking document ID {doc_id}: {str(e)}")
        return existing_ids
    
    
    def authenticate_drive_with_service_account(self):
        service_account_info = st.secrets["google_drive"]["service_account_info"]
        service_account_info = json.loads(service_account_info)
        creds = service_account.Credentials.from_service_account_info(
            service_account_info, scopes=SCOPES)
        return build('drive', 'v3', credentials=creds)


    def download_file_from_drive(self, service, file_id, file_name):
        os.makedirs(TEMP_DOWNLOAD_DIR, exist_ok=True)
        destination = os.path.join(TEMP_DOWNLOAD_DIR, file_name)
        request = service.files().get_media(fileId=file_id)
        with io.FileIO(destination, 'wb') as fh:
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
                print(f"Fetching {int(status.progress() * 100)}% of {file_name}.")
        return destination
    

    def list_files_in_drive(self, service, folder_id):
        """Returns list of all files in a given folder and its subfolders each
        as dict with values:
        1.mimeType
        2. id
        3. name
        """
        files = []
        folder_queue = [folder_id]
        while folder_queue:
            current_folder_id = folder_queue.pop(0)
            query = f"'{current_folder_id}' in parents"
            results = service.files().list(q=query, fields="files(id, name, mimeType)").execute()
            items = results.get('files', [])
            for item in items:
                if item['mimeType'] == 'application/vnd.google-apps.folder':
                    folder_queue.append(item['id'])
                else:
                    files.append(item)
        return files

    def process_and_add_documents_from_drive(self, folder_id=None):
        print(folder_id)
        service = self.authenticate_drive_with_service_account()
        # folder_id = folder_id if folder_id else self.drive_folder_id
        try:
            files = self.list_files_in_drive(service, folder_id=folder_id)
        except Exception as e:
            st.error("Folder ID invalid or not given access")
            return

        if not files:
            print("No documents found in the specified folder.")
            st.warning("No documents found in the specified folder.")
            return

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)

        # Step 1: Extract filenames (without extensions) to use as IDs
        filenames = [os.path.splitext(file['name'])[0] for file in files]
        
        # Step 2: Check if the document IDs already exist in Pinecone
        existing_ids = self.check_existing_docs_by_id(filenames)

        # Step 3: Filter out files that already exist in Pinecone
        new_files = [file for file in files if os.path.splitext(file['name'])[0] not in existing_ids]

        if not new_files:
            print("No new documents to add.")
            st.success("No new documents to add.")
            return
        else:
            st.success("New files found. Fetching:")
        # Step 4: Process and add only the new files to Pinecone
        for file in new_files:
            file_id = file['id']
            file_name = file['name']
            st.write(f"Fetching file from Drive: {file_name}")

            # Download the file temporarily
            downloaded_path = self.download_file_from_drive(service, file_id, file_name)

            # Load the file using the appropriate loader
            if file_name.endswith(".pdf"):
                loader = PyMuPDFLoader(file_path=downloaded_path)
            elif file_name.endswith(".docx"):
                loader = Docx2txtLoader(file_path=downloaded_path)
            elif file_name.endswith(".txt"):
                loader = TextLoader(file_path=downloaded_path)
            else:
                print(f"Unsupported file format: {file_name}")
                continue

            filename = os.path.splitext(file_name)[0]
            # print(f"Processing document: {filename}")
            st.write(f"Processing document: {filename}")
            
            # Add document-level dummy vector
            dummy_vector = [1.0] * 1536  # Ensure vector values are floats
            self.index.upsert(vectors=[(filename, dummy_vector)])  # Add document-level vector for tracking

            # Load and split the document into chunks
            file_docs = loader.load()
            chunks = text_splitter.split_documents(file_docs)
            print(f"Processed {len(chunks)} chunks from document {filename}")

            # Generate IDs for the chunks
            ids = [f"{filename}_chunk_{i}" for i, _ in enumerate(chunks)]
            
            # Add chunk-level vectors using add_documents
            self.vector_store.add_documents(documents=chunks, ids=ids)
            st.write(f"Document processing and vector store update complete for {filename}.")
            print(f"Document processing and vector store update complete for {filename}.")

            # Clean up the downloaded file
            os.remove(downloaded_path)


    def process_and_add_documents_from_local(self):
        """
        Process new PDF and DOCX and Text documents from the directory and add them to Pinecone.
        Only new documents (not already in Pinecone) are processed.
        """
        # Step 1: Get all PDF and DOCX file paths from the directory and its subdirectories
        file_paths = self.get_file_paths_from_directory_and_subdirectories()

        # Step 2: Extract filenames (without extensions) to use as IDs
        filenames = [os.path.splitext(os.path.basename(path))[0] for path in file_paths]

        # Step 3: Check if the document IDs already exist in Pinecone
        existing_ids = self.check_existing_docs_by_id(filenames)

        # Step 4: Filter out paths of files that already exist
        new_file_paths = [path for path in file_paths if os.path.splitext(os.path.basename(path))[0] not in existing_ids]

        if not new_file_paths:
            print("No new documents to add.")
            return

        # Initialize text splitter for chunking documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)

        # Step 5: Process new files
        for file_path in new_file_paths:
            # Load PDF or DOCX file
            if file_path.endswith(".pdf"):
                loader = PyMuPDFLoader(file_path=file_path)
                print(f"Processing PDF: {file_path}")
            elif file_path.endswith(".docx"):
                loader = Docx2txtLoader(file_path=file_path)
                print(f"Processing DOCX: {file_path}")
            elif file_path.endswith(".txt"):
                loader = TextLoader(file_path=file_path)
            else:
                print(f"Unsupported file format: {file_path}")
                continue
            filename = os.path.splitext(os.path.basename(file_path))[0]
            print(filename)
            # Step 5: Add document-level dummy vector
            dummy_vector = [1.0] * 1536  # Ensure vector values are floats
            self.index.upsert(vectors=[(filename, dummy_vector)])  # Add document-level vector for tracking

            # Step 6: Load and split the document into chunks
            file_docs = loader.load()
            chunks = text_splitter.split_documents(file_docs)
            print(f"Processed {len(chunks)} chunks from document {filename}")

            # Generate IDs for the chunks
            ids = [f"{filename}_chunk_{i}" for i, _ in enumerate(chunks)]
            
            # Step 7: Add chunk-level vectors using add_documents
            self.vector_store.add_documents(documents=chunks, ids=ids)
            print("Document processing and vector store update complete.")

    def get_podcasts(self, rss_url):
        """
        Fetches podcasts from an RSS feed and extracts MP3 URLs.
        
        Parameters:
        self.rss_url (str): URL of the RSS feed.
        
        Returns:
        list: A list of dictionaries with new podcast details including title, published date, and MP3 URL.
        """
        print("Getting RSS Feed")
        feed = feedparser.parse(rss_url)
        new_podcasts = []

        for entry in feed.entries:
            mp3_url = next(
                (enclosure.href for enclosure in entry.get('enclosures', [])
                if enclosure.type == 'audio/mpeg'), None
            )
            title = entry.title.strip()
            title = unicodedata.normalize('NFKD', title).encode('ascii', 'ignore').decode('ascii')
            if mp3_url:
                podcast_info = {
                    'title': title,
                    'published': entry.published.strip() if 'published' in entry else entry.updated,
                    'mp3_url': mp3_url
                }
                new_podcasts.append(podcast_info)

        return new_podcasts

    def split_audio_with_pydub(input_path, output_dir, chunk_duration=1200):
        """
        Splits the audio file into chunks using pydub, with parallel processing.
        """
        os.makedirs(output_dir, exist_ok=True)
        audio = AudioSegment.from_file(input_path)
        total_duration = len(audio)  # Duration in milliseconds

        def save_chunk(start_time, end_time, chunk_index):
            chunk = audio[start_time:end_time]
            chunk_path = os.path.join(output_dir, f"chunk_{chunk_index:03d}.mp3")
            chunk.export(chunk_path, format="mp3")

        # Split audio into chunks and save each in parallel
        with ThreadPoolExecutor() as executor:
            futures = []
            for i in range(0, total_duration, chunk_duration * 1000):  # chunk_duration is in seconds, convert to ms
                start_time = i
                end_time = min(i + chunk_duration * 1000, total_duration)
                chunk_index = i // (chunk_duration * 1000)
                futures.append(executor.submit(save_chunk, start_time, end_time, chunk_index))

            # Wait for all chunks to finish processing
            for future in futures:
                future.result()


    def split_audio_with_moviepy(self, input_path, output_dir, chunk_duration=1200):
        """
        Splits the audio file into chunks using moviepy.
        """
        os.makedirs(output_dir, exist_ok=True)
        audio_clip = AudioFileClip(input_path)
        total_duration = audio_clip.duration

        # Split audio into chunks and save each as a separate file
        for i in range(0, int(total_duration), chunk_duration):
            start_time = i
            end_time = min(i + chunk_duration, total_duration)
            chunk = audio_clip.subclip(start_time, end_time)
            chunk_path = os.path.join(output_dir, f"chunk_{i//chunk_duration:03d}.mp3")
            chunk.write_audiofile(chunk_path, codec='mp3')

    def transcribe_chunk(self, chunk_path):
        """
        Transcribes a single audio chunk using OpenAI Whisper API.
        """
        with open(chunk_path, "rb") as audio_file:
            print("Converting to text ", chunk_path)
            transcription = self.client.audio.transcriptions.create(
                file=audio_file,
                model="whisper-1",
                response_format="verbose_json",
                timestamp_granularities=["segment"]
            )
        os.remove(chunk_path)  # Clean up chunk file after transcription
        return transcription.text  # Collect only text

    def process_podcast_audio(self, audio_url, chunk_duration=1200):
        """
        Downloads, segments, and transcribes an audio file using OpenAI Whisper API.
        """
        output_dir = os.path.join(TEMP_DOWNLOAD_DIR, "chunks")
        self.split_audio_with_moviepy(audio_url, output_dir, chunk_duration=chunk_duration)

        transcripts = []

        # Use ThreadPoolExecutor to transcribe each chunk in parallel
        with ThreadPoolExecutor() as executor:
            # Submit all transcription tasks in sorted order and collect results in the same order
            transcripts = list(executor.map(
                lambda chunk_filename: self.transcribe_chunk(os.path.join(output_dir, chunk_filename)),
                sorted(os.listdir(output_dir))
            ))

        os.rmdir(output_dir)  # Remove the chunks directory after all transcriptions are done
        return " ".join(transcripts)


    def add_podcast_to_index(self, podcast_id, transcript):
        """
        Splits the transcript into smaller chunks, converts each to a Document,
        and adds them to the Pinecone index.
        """
        self.index.upsert([(podcast_id, [1.0] * self.dimensions)])

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)
        chunks = text_splitter.split_text(transcript)
        documents = [Document(page_content=chunk, metadata={"source": podcast_id}) for chunk in chunks]
        self.vector_store.add_documents(documents=documents, ids=[f"{podcast_id}_chunk_{i}" for i in range(len(documents))])

    def process_and_add_new_podcasts(self, rss_url, latest_n=-1, download=False):
        """
        Main method to retrieve, process, and add new podcasts from RSS feed.
        """
        print("Fetching podcasts from RSS feed...")
        st.success("Fetching podcasts from RSS feed...")
        rss_url = rss_url if rss_url else self.rss_url 
        podcasts = self.get_podcasts(rss_url)
        podcast_ids = [podcast["title"] for podcast in podcasts]
        st.success("Checking dupes for podcasts")
        existing_ids = self.check_existing_docs_by_id(podcast_ids)
        new_podcasts = [podcast for podcast in podcasts if podcast['title'] not in existing_ids][:latest_n]
        print(new_podcasts)
        transcripts = []
        if new_podcasts:
            st.success("New podcasts found.")
            for idx, podcast in enumerate(new_podcasts, start=1):
                podcast_id = podcast["title"]
                st.info("Making transcription..for {}".format(podcast_id))
                print(podcast_id)
                transcript = self.process_podcast_audio(podcast["mp3_url"])
                if not download:
                    self.add_podcast_to_index(podcast_id, transcript)
                    st.success(f"Podcast '{podcast_id}' processed and added to Pinecone. {idx} out of {latest_n}")
                else:
                    transcripts.append((podcast_id,transcript))
            if download:
                self.create_zip_download(transcripts)
        else:
            st.success("No new podcasts to be ingested")
    
    @st.fragment
    def create_zip_download(self, transcripts):
        """
        Creates a zip file for all transcripts and provides a download button in Streamlit.
        """
        # Create a BytesIO buffer to hold the zip file in memory
        zip_buffer = io.BytesIO()

        # Create a zip file and add each transcript
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
            for podcast_id, transcript in transcripts:
                # Create a text file for each transcript inside the zip file
                transcript_filename = f"{podcast_id}.txt"
                zf.writestr(transcript_filename, transcript)
        
        # Seek to the beginning of the BytesIO buffer so it can be read
        zip_buffer.seek(0)

        # Provide a download button in Streamlit for the zip file
        st.download_button(
            label="Download All Transcripts as ZIP",
            data=zip_buffer,
            file_name="transcripts.zip",
            mime="application/zip"
        )
    
# Example usage:
if __name__ == "__main__": ## TESTING ##    
    # folder_id = "1ptf4zFnw0Lu5sJMMDIqwY2WxDVj29kz1"
    processor = DocumentProcessor(INDEX_NAME)
    # processor.process_and_add_documents_from_drive()
    # processor.process_and_add_documents_from_local()
    text = processor.process_podcast_audio("https://chrt.fm/track/DBDAF8/cdn.simplecast.com/audio/a40ab607-6c05-44d6-b237-5cd4abd47ac6/episodes/5b62ad45-1287-402e-b50a-0dcf220a1760/audio/2adb5029-5fb7-4afe-bec3-4a5f48358581/default_tc.mp3?aid=rss_feed&feed=XFfCG1w8")
    # existing_ids = processor.check_existing_docs_by_id(files)

    # processor.process_and_add_documents_from_local()
    # filepaths = processor.get_file_paths_from_directory_and_subdirectories()
    # print(filepaths)
    # processor.extract_filenames_from_vector_ids(filenames)
    #     print(processor.check_existing_docs_by_id(filenames))
